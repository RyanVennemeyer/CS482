{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "482FinalProj.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/serene23/NLP-Project/blob/main/482FinalProj.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mwf-R3Ag9co0"
      },
      "source": [
        "%%capture\n",
        "!pip install datasets\n",
        "!pip install git+https://github.com/huggingface/transformers.git\n",
        "!pip install flax\n",
        "!pip install git+https://github.com/deepmind/optax.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing all the libraries necessary for the project\n",
        "import os\n",
        "import time\n",
        "import jax\n",
        "import flax\n",
        "import optax\n",
        "import datasets\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "from jax import jit\n",
        "import jax.numpy as jnp\n",
        "import tensorflow as tf\n",
        "from flax.training import train_state\n",
        "from itertools import chain\n",
        "from tqdm.notebook import tqdm\n",
        "from typing import Callable\n",
        "from flax import traverse_util\n",
        "from datasets import load_dataset, load_metric ,Dataset,list_metrics,load_from_disk\n",
        "from flax.training.common_utils import get_metrics, onehot, shard, shard_prng_key\n",
        "from transformers import FlaxAutoModelForSequenceClassification, AutoConfig, AutoTokenizer, BertTokenizer\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
      ],
      "metadata": {
        "id": "rlAJ75IFk_I1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUCQEB-l98VI"
      },
      "source": [
        "import jax.tools.colab_tpu\n",
        "jax.tools.colab_tpu.setup_tpu()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jax.local_devices()"
      ],
      "metadata": {
        "id": "ku5Dt8OlLSCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "task = \"mnli\"\n",
        "model_checkpoint = \"bert-base-cased\"\n",
        "per_device_batch_size = 4"
      ],
      "metadata": {
        "id": "A5N4yoHNflPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fm_XuXi9b4U"
      },
      "source": [
        "from datasets import load_dataset, load_metric"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "prLZ-5zwbtfX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "actual_task = \"mnli\"\n",
        "raw_train = load_dataset(\"csv\", data_files={'train': ['/content/train.csv']}) #change this path to your desired file\n",
        "raw_dataset = raw_train[\"train\"].train_test_split(0.2)\n",
        "metric = load_metric('glue', actual_task) #load metrics needed for our task, can change second parameter to accomodate your task"
      ],
      "metadata": {
        "id": "XZMalfmlK97z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_dataset"
      ],
      "metadata": {
        "id": "4djWTBrNLCUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "    \n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint) #tokenizing with bert-base-cased"
      ],
      "metadata": {
        "id": "usvsPL3edbJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "task_to_keys = {   #we selected mnli for our project\n",
        "    \"cola\": (\"sentence\", None),\n",
        "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
        "    \"mnli-mm\": (\"premise\", \"hypothesis\"),\n",
        "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
        "    \"qnli\": (\"question\", \"sentence\"),\n",
        "    \"qqp\": (\"question1\", \"question2\"),\n",
        "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
        "    \"sst2\": (\"sentence\", None),\n",
        "    \"stsb\": (\"sentence1\", \"sentence2\"),\n",
        "    \"wnli\": (\"sentence1\", \"sentence2\"),\n",
        "} "
      ],
      "metadata": {
        "id": "sd4iXVCcdiUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence1_key, sentence2_key = task_to_keys[task]\n",
        "\n",
        "def preprocess_function(examples):    #https://www.kaggle.com/code/nilaychauhan/jigsaw-toxic-comment-classification-using-jax-flax\n",
        "    texts = (\n",
        "        (examples[sentence1_key],) if sentence2_key is None else (examples[sentence1_key], examples[sentence2_key])\n",
        "    )\n",
        "    processed = tokenizer(*texts, padding=\"max_length\", max_length=128, truncation=True)\n",
        "    \n",
        "    processed[\"labels\"] = examples[\"label\"]\n",
        "    return processed    "
      ],
      "metadata": {
        "id": "5OPKYTHedjSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset = raw_dataset.map(preprocess_function, batched=True, remove_columns=raw_dataset[\"train\"].column_names)"
      ],
      "metadata": {
        "id": "_0bA5KHadmHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "yihWOwiFpmDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = tokenized_dataset[\"train\"]\n"
      ],
      "metadata": {
        "id": "Jdj1QDwAeT7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset"
      ],
      "metadata": {
        "id": "MOgQmetXf5ZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_dataset = tokenized_dataset[\"test\"]"
      ],
      "metadata": {
        "id": "sYq10AAsfYS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import FlaxAutoModelForSequenceClassification, AutoConfig\n",
        "\n",
        "num_labels = 3 \n",
        "seed = 0\n",
        "\n",
        "config = AutoConfig.from_pretrained(model_checkpoint, num_labels=num_labels)\n",
        "model = FlaxAutoModelForSequenceClassification.from_pretrained(model_checkpoint, config=config, seed=seed)"
      ],
      "metadata": {
        "id": "UsIPeq4HgDFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mn1GdGpipfWK"
      },
      "source": [
        "import flax\n",
        "import jax\n",
        "import optax\n",
        "\n",
        "from itertools import chain\n",
        "from tqdm.notebook import tqdm\n",
        "from typing import Callable\n",
        "\n",
        "import jax.numpy as jnp\n",
        "\n",
        "from flax.training.common_utils import get_metrics, onehot, shard, shard_prng_key\n",
        "from flax.training import train_state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_train_epochs = 2 #matched epochs with our TensorFlow run\n",
        "learning_rate = 2e-5"
      ],
      "metadata": {
        "id": "Lfz0JjNJgosg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_batch_size = per_device_batch_size * jax.local_device_count()\n",
        "print(\"The overall batch size (both for training and eval) is\", total_batch_size)"
      ],
      "metadata": {
        "id": "rumSzvR6gvVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_train_steps = len(train_dataset) // total_batch_size * num_train_epochs\n",
        "\n",
        "learning_rate_function = optax.linear_schedule(init_value=learning_rate, end_value=0, transition_steps=num_train_steps)"
      ],
      "metadata": {
        "id": "ILXr1uCTgx3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainState(train_state.TrainState):   #https://www.kaggle.com/code/nilaychauhan/jigsaw-toxic-comment-classification-using-jax-flax\n",
        "    logits_function: Callable = flax.struct.field(pytree_node=False)  \n",
        "    loss_function: Callable = flax.struct.field(pytree_node=False)"
      ],
      "metadata": {
        "id": "YLzhTQUug2mf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decay_mask_fn(params):\n",
        "    flat_params = traverse_util.flatten_dict(params)\n",
        "    flat_mask = {path: (path[-1] != \"bias\" and path[-2:] != (\"LayerNorm\", \"scale\")) for path in flat_params}\n",
        "    return traverse_util.unflatten_dict(flat_mask)"
      ],
      "metadata": {
        "id": "eOKFfrVyg4sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def adamw(weight_decay):  #https://www.kaggle.com/code/anasofiauzsoy/tutorial-notebook/notebook\n",
        "    return optax.adamw(learning_rate=learning_rate_function, b1=0.9, b2=0.999, eps=1e-6, weight_decay=weight_decay,mask=decay_mask_fn)"
      ],
      "metadata": {
        "id": "TutAlVyGg5SZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weight_decay = 1e-2\n",
        "adamw = adamw(weight_decay)  "
      ],
      "metadata": {
        "id": "Q77InAz4j3ds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from jaxlib.xla_extension.jax_jit import jit\n",
        "\n",
        "\n",
        "def loss_function(logits, labels):\n",
        "  xentropy = optax.softmax_cross_entropy(logits, onehot(labels, num_classes=num_labels))\n",
        "  return jnp.mean(xentropy)\n",
        "    \n",
        "def eval_function(logits):\n",
        "    return logits.argmax(-1)"
      ],
      "metadata": {
        "id": "84vFc4Cug8Af"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state = TrainState.create(\n",
        "    apply_fn=model.__call__,\n",
        "    params=model.params,\n",
        "    tx=adamw,\n",
        "    logits_function=eval_function,\n",
        "    loss_function=loss_function,\n",
        ")\n"
      ],
      "metadata": {
        "id": "zqUrSZFfg-Rv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(state, batch, dropout_rng):\n",
        "    targets = batch.pop(\"labels\")\n",
        "    dropout_rng, new_dropout_rng = jax.random.split(dropout_rng)\n",
        "\n",
        "    def loss_function(params):\n",
        "        logits = state.apply_fn(**batch, params=params, dropout_rng=dropout_rng, train=True)[0]\n",
        "        loss = state.loss_function(logits, targets)\n",
        "        return loss\n",
        "\n",
        "    grad_function = jax.value_and_grad(loss_function)\n",
        "    loss, grad = grad_function(state.params)\n",
        "    grad = jax.lax.pmean(grad, \"batch\")\n",
        "    new_state = state.apply_gradients(grads=grad)\n",
        "    metrics = jax.lax.pmean({\"loss\": loss, \"learning_rate\": learning_rate_function(state.step)}, axis_name=\"batch\")\n",
        "    return new_state, metrics, new_dropout_rng"
      ],
      "metadata": {
        "id": "L1clKC2bhtak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parallel_train_step = jax.pmap(train_step, axis_name=\"batch\", donate_argnums=(0,))"
      ],
      "metadata": {
        "id": "AoADkycyhyop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_step(state, batch):\n",
        "    logits = state.apply_fn(**batch, params=state.params, train=False)[0]\n",
        "    return state.logits_function(logits)"
      ],
      "metadata": {
        "id": "5bw0gJcHh5vs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parallel_eval_step = jax.pmap(eval_step, axis_name=\"batch\")"
      ],
      "metadata": {
        "id": "Gn1cROrzh0NC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def glue_train_data_loader(rng, dataset, batch_size):\n",
        "    steps_per_epoch = len(dataset) // batch_size\n",
        "    perms = jax.random.permutation(rng, len(dataset))\n",
        "    perms = perms[: steps_per_epoch * batch_size]  # Skip incomplete batch.\n",
        "    perms = perms.reshape((steps_per_epoch, batch_size))\n",
        "\n",
        "    for perm in perms:\n",
        "        batch = dataset[perm]\n",
        "        batch = {k: jnp.array(v) for k, v in batch.items()}\n",
        "        batch = shard(batch)\n",
        "\n",
        "        yield batch"
      ],
      "metadata": {
        "id": "ZAkOobQTh73m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def glue_eval_data_loader(dataset, batch_size):\n",
        "    for i in range(len(dataset) // batch_size):\n",
        "        batch = dataset[i * batch_size : (i + 1) * batch_size]\n",
        "        batch = {k: jnp.array(v) for k, v in batch.items()}\n",
        "        batch = shard(batch)\n",
        "\n",
        "        yield batch"
      ],
      "metadata": {
        "id": "ceqvq7P5h-jk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state = flax.jax_utils.replicate(state)"
      ],
      "metadata": {
        "id": "kvXwlqfYiAUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rng = jax.random.PRNGKey(seed)\n",
        "dropout_rngs = jax.random.split(rng, jax.local_device_count())"
      ],
      "metadata": {
        "id": "1XkJ7BwXlHkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, epoch in enumerate(tqdm(range(1, num_train_epochs + 1), desc=f\"Epoch ...\", position=0, leave=True)):   #https://www.kaggle.com/code/nilaychauhan/jigsaw-toxic-comment-classification-using-jax-flax\n",
        "    rng, input_rng = jax.random.split(rng)\n",
        "\n",
        "    # train\n",
        "    with tqdm(total=len(train_dataset) // total_batch_size, desc=\"Training...\", leave=False) as progress_bar_train:\n",
        "      for batch in glue_train_data_loader(input_rng, train_dataset, total_batch_size):\n",
        "        state, train_metrics, dropout_rngs = parallel_train_step(state, batch, dropout_rngs)\n",
        "        progress_bar_train.update(1)\n",
        "\n",
        "    # evaluate\n",
        "    with tqdm(total=len(eval_dataset) // total_batch_size, desc=\"Evaluating...\", leave=False) as progress_bar_eval:\n",
        "      for batch in glue_eval_data_loader(eval_dataset, total_batch_size):\n",
        "          labels = batch.pop(\"labels\")\n",
        "          predictions = parallel_eval_step(state, batch)\n",
        "          metric.add_batch(predictions=chain(*predictions), references=chain(*labels))\n",
        "          progress_bar_eval.update(1)\n",
        "\n",
        "    eval_metric = metric.compute()\n",
        "\n",
        "    loss = round(flax.jax_utils.unreplicate(train_metrics)['loss'].item(), 3)\n",
        "    eval_score = round(list(eval_metric.values())[0], 3)\n",
        "    metric_name = list(eval_metric.keys())[0]\n",
        "\n",
        "    print(f\"{i+1}/{num_train_epochs} | Train loss: {loss} | Eval {metric_name}: {eval_score}\")"
      ],
      "metadata": {
        "id": "IBoPKZPPljoW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}